#!/usr/bin/python

# -*- mode: Python -*-

import h5py
import arf
import argparse
import numpy as np
import shutil


"""Copies data in arf file to new file with no chunking and compressed data"""

def get_datasets(group):
    for entry in group.itervalues():
        if isinstance(entry,h5py.Dataset):
            yield entry
        elif isinstance(entry,h5py.Group):
            for dataset in get_datasets(entry):
                yield dataset
        
        

def unchunk(chunked_file, new_file, compression_level, v):
    for dset in get_datasets(chunked_file):
        if v:
            print(dset)
        if dset.name == '/jill_log':
            if len([m for m in dset["message"]]) == 0:
                new_file.copy(dset,dset.name)
            else:
                dtype = np.dtype([('sec','<i8'), ('usec', '<i8'),
                                  ('message','S%d'%(max(len(m) for m in dset['message'])))])
                data = np.array(dset[:], dtype=dtype)
                new_file.create_dataset(dset.name,data=data, compression=compression_level)

        elif np.issubdtype(dset.dtype,(np.float,np.integer)):
            try:
                new_dset = new_file.create_dataset(dset.name,data=dset[:], compression=compression_level)
            except:
                print("Warning: Dataset %s cannot be read and will not be copied"%(dset.name))
                return False
            for key,value in dset.attrs.iteritems():
                new_dset.attrs[key] = value
        else:
            new_file.copy(dset,dset.name)
    new_file.attrs["olive-pressed"] = compression_level
    return True


def verify_file_contents(chunked_file, new_file, v):
    for dset in get_datasets(chunked_file):
        for key in dset.attrs:
            if key not in new_file[dset.name].attrs:
                print("attribute {} not an attribute of {}{}".format(key, new_file, dset.name))
                return False
            if isinstance(dset.attrs[key], (np.ndarray, np.generic)):
                if not np.array_equal(dset.attrs[key], new_file[dset.name].attrs[key]):
                    print("array attribute value {} not equal when copied to {}{}"
                          .format(key, new_file, dset.name))
                    return False
            elif dset.attrs[key] is not new_file[dset.name].attrs[key]:
                print("attribute value {} not equal when copied to {}{}"
                          .format(key, new_file, dset.name))
                return False
            if isinstance(dset, h5py.Dataset):
                if not  np.array_equal(dset.value, new_file[dset.name].value):
                    print("Dataset value {} not equal when copied to {}{}"
                          .format(key, new_file, dset.name))
                    return False
    return True


def already_pressed(fname, compression_level):
    with h5py.File(fname, "r") as chunked_file:
        if ("olive-pressed" in chunked_file.attrs and
            compression_level >= chunked_file.attrs["olive-pressed"]):
                return True
        else:
            return False



def main(chunked_arfs, compression_level, v, replace):
    
    for chunked_arf in chunked_arfs:
        new_arf = chunked_arf + ".olive-press.arf"
        copy_success, copy_double_check = False, False
        if already_pressed(chunked_arf, compression_level):
            if v:
                print("{} already compressed, skipping..."
                      .format(chunked_arf))
            continue
        if v:
            print(chunked_arf)
        with h5py.File(chunked_arf,'r+') as chunked_file,\
             h5py.File(new_arf,'w-') as new_file:
            copy_success = unchunk(chunked_file, new_file, compression_level, v)
            if not copy_success:
                print("copy of {} failed, compare with {}".format(chunked_arf, new_arf))
                continue
            copy_double_check = verify_file_contents(chunked_file, new_file, v)
        if not copy_double_check:
            print("copy of {} double check failed, compare with {}".format(chunked_arf, new_arf))
            continue
        
        elif replace:
            shutil.move(new_arf, chunked_arf)
            if v:
                print("{} replaced with {}".format(chunked_arf, new_arf))


if __name__=='__main__':
    compression_default = 9
    p = argparse.ArgumentParser(prog="unchunk",
                                description="Copies data in arf file to new file with no chunking")
    p.add_argument("chunked_arf", help="input Arf file[s]", nargs="+")
    p.add_argument("-c", "--compression",
                   help="compression level, 0 = none (fast), 9 = high (slow), default: {}".format(compression_default),
                   type=int, default=compression_default)
    p.add_argument("-v", "--verbose", action="store_true", default=False)
    p.add_argument("--replace", help="replaces old arf file with new, compressed arf file", default=False,
                   action="store_true")
    options = p.parse_args()
    main(options.chunked_arf, options.compression, options.verbose, options.replace)
